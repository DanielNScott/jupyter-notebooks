{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prob 2.2 ###\n",
    "How does the softmax action selection method using the Gibbs distribution fare on the 10-armed bandit task? Implement the method and run it at several temperatures to produce graphs similar to those in Figure 2.1. To verify your code, first implement the $\\epsilon$-greedy methods and reproduce some specific aspect of the results in Figure 2.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized bandit with params:\n",
      "Expected action values: [0, 1  , 2]\n",
      "Standard deviations   : [1, 0.5, 3]\n",
      " \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQOIIegBIHEEPAIn7f8Y8oDwP/xzpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f16d43c4160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#-----------------------------------#\n",
    "# Create a slot machine             #\n",
    "#-----------------------------------#\n",
    "\n",
    "# 10-armed bandit specification\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "class Gaussian_Bandit(object):\n",
    "    \"\"\"gaussian_bandit(n_arms, action_values) returns a callable n-armed gaussian bandit.\n",
    "\n",
    "    Args:\n",
    "        action_values (list): A list of action-value-expectations. \n",
    "\n",
    "    Returns:\n",
    "        object: A gaussian_bandit object with the 'do_action' method.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, action_means, action_sdevs):\n",
    "        self.action_means = action_means\n",
    "        self.action_sdevs = action_sdevs\n",
    "        self.n_arms       = len(action_means)\n",
    "        \n",
    "    def do_action(self, action_number):\n",
    "        mu    = self.action_means[action_number]\n",
    "        sigma = self.action_sdevs[action_number]\n",
    "        return np.random.normal(mu, sigma)\n",
    "        \n",
    "a_bandit = Gaussian_Bandit([0, 1, 2],[1, 0.5, 3])\n",
    "\n",
    "print('Initialized bandit with params:')\n",
    "print('Expected action values: [0, 1  , 2]')\n",
    "print('Standard deviations   : [1, 0.5, 3]')\n",
    "print(' ')\n",
    "\n",
    "#-----------------------------#\n",
    "# Verify proper functionality #\n",
    "#-----------------------------#\n",
    "results = np.zeros([3,300])\n",
    "for index in range(0,900):\n",
    "    \n",
    "    # Cycle through slots\n",
    "    slot      = index % 3\n",
    "    slot_iter = int(np.floor(index/3))\n",
    "        \n",
    "    results[slot, slot_iter] = a_bandit.do_action(slot)\n",
    "    \n",
    "# Plot results\n",
    "bins = np.linspace(-10,15,50)\n",
    "plt.hist(results[0,:], bins, alpha=0.4, label='0')\n",
    "plt.hist(results[1,:], bins, alpha=0.4, label='0')\n",
    "plt.hist(results[2,:], bins, alpha=0.4, label='0')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Epsilon_Greedy_Learner(object):\n",
    "    \"\"\"Epsilon_Greedy_Learner returns a callable epsilon greedy learner with sample-average action value learning.\n",
    "    \n",
    "    Args:\n",
    "        epsilon (real): Fraction of the time it chooses non-greedily.\n",
    "        \n",
    "    Returns:\n",
    "        object: The learner.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, epsilon, n_actions):\n",
    "        self.epsilon   = epsilon\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        # Init action values and action counters to zero\n",
    "        self.learned_action_values  = [0] * n_actions\n",
    "        self.count_of_actions_taken = [0] * n_actions\n",
    "        \n",
    "    def note_action_reward_pair(self, action, reward):\n",
    "        count   = self.count_of_actions_taken[action]\n",
    "        val_est = self.learned_action_values[action]\n",
    "        \n",
    "        new_est = val_est * count / (count + 1) + reward / (count + 1)\n",
    "        \n",
    "        self.learned_action_values[action]  = new_est\n",
    "        self.count_of_actions_taken[action] = count + 1\n",
    "        \n",
    "    def select_action(self):\n",
    "        val_ests    = self.learned_action_values\n",
    "        be_greedy   = np.random.uniform() > self.epsilon\n",
    "        best_action = val_ests.index(max(val_ests))\n",
    "        \n",
    "        if be_greedy:\n",
    "            action_to_take = best_action\n",
    "        else:            \n",
    "            non_greedy_list = list(range(0, best_action)) + list(range(best_action+1, self.n_actions))\n",
    "            action_selector = int( np.floor(np.random.uniform() * (self.n_actions-1)) )\n",
    "            action_to_take  = non_greedy_list[action_selector]\n",
    "            \n",
    "        return action_to_take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create some bandits and learn how to play them optimally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 0, Reward: -0.23, Money:  8.77, Time: 19\n",
      "Action: 1, Reward:  3.39, Money:  11.16, Time: 18\n",
      "Action: 1, Reward:  1.99, Money:  12.15, Time: 17\n",
      "Action: 1, Reward:  3.53, Money:  14.68, Time: 16\n",
      "Action: 1, Reward:  3.18, Money:  16.86, Time: 15\n",
      "Action: 1, Reward:  4.92, Money:  20.78, Time: 14\n",
      "Action: 1, Reward:  3.02, Money:  22.80, Time: 13\n",
      "Action: 1, Reward:  4.29, Money:  26.09, Time: 12\n",
      "Action: 1, Reward:  2.96, Money:  28.06, Time: 11\n",
      "Action: 1, Reward:  3.83, Money:  30.89, Time: 10\n",
      "Action: 1, Reward:  1.52, Money:  31.41, Time: 9\n",
      "Action: 1, Reward:  2.28, Money:  32.70, Time: 8\n",
      "Action: 1, Reward:  1.98, Money:  33.67, Time: 7\n",
      "Action: 1, Reward:  2.05, Money:  34.72, Time: 6\n",
      "Action: 1, Reward:  2.33, Money:  36.05, Time: 5\n",
      "Action: 1, Reward:  1.10, Money:  36.15, Time: 4\n",
      "Action: 1, Reward:  4.08, Money:  39.23, Time: 3\n",
      "Action: 0, Reward:  2.51, Money:  40.74, Time: 2\n",
      "Action: 1, Reward:  1.87, Money:  41.61, Time: 1\n",
      "Action: 1, Reward:  3.26, Money:  43.87, Time: 0\n"
     ]
    }
   ],
   "source": [
    "# Set up some easy bandits and a harder one\n",
    "really_easy_bandit = Gaussian_Bandit([0, 3], [1, 1])\n",
    "pretty_easy_bandit = Gaussian_Bandit([0, 1], [1, 1])\n",
    "\n",
    "easyish_bandit = Gaussian_Bandit([0, 0.75, 1.5], [1, 1])\n",
    "hardish_bandit = Gaussian_Bandit([0, 0.25, 0.5], [1, 1])\n",
    "\n",
    "\n",
    "# Create some greedy learners\n",
    "myopic_monkey = Epsilon_Greedy_Learner(0.05, 2)\n",
    "\n",
    "# Give learners lifelike attributes\n",
    "myopic_monkey.money = 10\n",
    "myopic_monkey.time  = 20\n",
    "\n",
    "\n",
    "# Monkey solvency function\n",
    "def monkey_has_time_and_money(monkey):\n",
    "    return monkey.money > 0 and monkey.time > 0\n",
    "\n",
    "\n",
    "# Let's lern them bandits!\n",
    "while monkey_has_time_and_money(myopic_monkey):\n",
    "    \n",
    "    action = myopic_monkey.select_action()\n",
    "    reward = really_easy_bandit.do_action(action)\n",
    "    \n",
    "    myopic_monkey.time  = myopic_monkey.time  - 1\n",
    "    myopic_monkey.money = myopic_monkey.money - 1 + reward\n",
    "    \n",
    "    myopic_monkey.note_action_reward_pair(action, reward)\n",
    "    \n",
    "    time  = myopic_monkey.time\n",
    "    money = myopic_monkey.money\n",
    "    print('Action: %d, Reward: % 3.2f, Money: % 4.2f, Time: %d' % (action, reward, money, time))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
